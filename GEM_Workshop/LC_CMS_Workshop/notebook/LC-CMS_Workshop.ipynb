{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb15b68-4e77-44ec-a2b0-36c44a8845b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Land Cover Continuous Monitoring Service (LC-CMS) Workshop\n",
    "![\"notebook/images/lccms_pipeline.PNG\"](images/lccms_pipeline.png)\n",
    "\n",
    "# A : Pre-Requirsites\n",
    "- Docker engine >= 10.0\n",
    "- QGIS : To visualize the Predictions & Results. Template for the project is provided in this repository.\n",
    "- Local disk space ~ 3GB\n",
    "- RAM >= 4 GB\n",
    "- Optional things :\n",
    "    - SentinelHub Account( If you want to visualize using BYOC services / EOBrowser). This will also require an S3 bucket which should be accessible using the SH. This will not be covered in this workshop but links will be provided so, that you can do it yourself.\n",
    "    \n",
    "## A.1 : Dataset for the Notebook (***Very Important***) :\n",
    "In order to run this Notebook, we'll need some dataset on which we can perform Land Classification. We'll use the dataset already prepared for Paris region for the purpose of this exercise, so **please make sure to download it before the workshop using following steps** : \n",
    "1. Download the data from the following download link : \n",
    "    https://cloud.sinergise.com/s/TJnrDjbT2tpiGA2\n",
    "\n",
    "     Alternatively, you can use following wget command : \n",
    "\n",
    "`wget https://cloud.sinergise.com/s/TJnrDjbT2tpiGA2/download/paris_run.zip -O paris_run.zip`\n",
    "\n",
    "2. The first step will download a **paris_run.zip** file. You can unzip it on your machine for now. \n",
    "\n",
    "**You will need this later while running the docker image, where you'll mount this directory to the docker container.**\n",
    "\n",
    "The downloaded dataset `paris_run` should have data for two years **2020 & 2021** with following structure : \n",
    "\n",
    "![\"notebook/images/paris_dataset_view.png\"](images/paris_dataset_view.png)\n",
    "\n",
    "## A.2 : QGIS Template for Visualizing the results\n",
    "There is QGIS directly in the repository, which contains the project file => **lc_cms_reslut_viewer.qgz**. \n",
    "Import it as a project in QGIS so that you can visualize the results later on.\n",
    "\n",
    "\n",
    "# B : How to run this notebook ?\n",
    "## B.1 : If you're using Docker :\n",
    "There is an associated Dockerfile in this repository, which installs all the necessary packages required to run this notebook. \n",
    "\n",
    "Additionaly, once the dockerfile is run, it also spins up a jupyter notebook in the container which can be accessed using a public port.\n",
    "1. As a first step, you can build the docker image by going to the home directory using command :\n",
    "\n",
    "`\n",
    "docker build -t gem-workshop:latest .\n",
    "`\n",
    "\n",
    "2. Next run the docker image which starts a jupyter notebook (Please make sure the port that you are using is not being using by some other process):\n",
    "\n",
    "`\n",
    "docker run -p 8888:8888 -v $(pwd):/gem-workshop -v $paris_run:/gem-workshop/paris_run -t gem-workshop:latest\n",
    "`\n",
    "\n",
    "**Note : `$paris_run` must be replaced by path to directly where you have downloaded the data as mentioned in section pre-requisites A.1**\n",
    "\n",
    "**If you're on windows You will have to replace $(pwd) with path to the checked out project directory.**\n",
    "\n",
    "3. You can now go to the link http://localhost:8888 and enter the password as **`gem`**. Then, open the jupyter notebook at location notebook/LC-CMS_Workshop.ipynb.\n",
    "\n",
    "\n",
    "## B.2 : If you are not using Docker :\n",
    "As an alternate if you are not using the Docker, you can install the following requirements/packages manually :\n",
    "- Python >= 3.9\n",
    "- Gdal binaries >= 3.23\n",
    "- Gdal-python >= 3.23\n",
    "- All the packages listed in requirements.txt of this repo :\n",
    "   - sentinelhub>=3.6.3\n",
    "   - eo-learn==1.3.1\n",
    "   - eo-grow==1.3.3\n",
    "   - geopandas==0.12.2\n",
    "   - scikit-learn==1.2.0\n",
    "   - scikit-image==0.19.3\n",
    "   - scipy==1.10.0\n",
    "   - juypyterlab\n",
    "   \n",
    "## Contact Details :\n",
    "### Ashish Dhiman\n",
    "#### Work Email : ashish.dhiman@tomtom.com\n",
    "#### Personal Email : ashish.dhiman.nith@gmail.com\n",
    "#### LinkedIn : https://www.linkedin.com/in/ashishdhiman89/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c7b2a-f370-4fc6-9909-588ddfc92514",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LC-CMS Pipeline Steps\n",
    "The following diagram shows the overview of the entire pipeline.\n",
    "The pipeline consists of different phases : \n",
    "1. Data Collection\n",
    "\n",
    "This phase consists of prepration of data that is required for the pipeline. \n",
    "\n",
    "2. Modelling\n",
    "3. Serving\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![\"Title\"](images/pipeline_overview.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484533b-8be3-4957-9455-2564e4a7f64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EOGrow 101 [Optional]\n",
    "* Before we begin, a quick summary of EOGrow Pipelines.\n",
    "* For more details, please check this repository [https://github.com/sentinel-hub/eo-grow](https://github.com/sentinel-hub/eo-grow). It contains some notebooks and examples to help you with it\n",
    "![\"images/eogrow_basics.png\"](images/eogrow_basics.png)\n",
    "\n",
    "**Global Config** : This configuration will be shared by all of the pipeline steps. More information can be found on the [ReadMe](https://github.com/sentinel-hub/eo-grow) of eo-grow repository.\n",
    "\n",
    "Using the global configuration in our `config_files/global_config.json` we have, will result in following directory structure :\n",
    "\n",
    "![\"Directory Structure\"](images/directory_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48344b7-4816-494d-9365-6e8d780f9846",
   "metadata": {},
   "source": [
    "**Before we begin, let's import some of the modules that we'll need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaec40b-d826-46c9-a98d-b1aa6f98375b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from eolearn.core import EOPatch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63871bea-696e-4f2c-8821-3d7436c47f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also, initialize some variable that we might need later\n",
    "RUN_DIR = '/gem-workshop/paris_run/2020'\n",
    "LABELS_ORIG = {1: 'forest',\n",
    "               2: 'grass',\n",
    "               3: 'farmland',\n",
    "               4: 'bare soil',\n",
    "               5: 'sand',\n",
    "               6: 'built-up',\n",
    "               7: 'permanent water',\n",
    "               8: 'intermittent water',\n",
    "               9: 'eternal snow'}\n",
    "INPUT_BANDS = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc8372-187c-46e2-8095-5c0e87b895cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Load Batch\n",
    "1. We use the step *eogrow.pipelines.download_batch.BatchDownloadPipeline* from the eo-grow library for this purpose. **For the purpose of this workshop, the data is already provided to you.**\n",
    "2. Generally, in this step we request the input data that we are interested in using the the SentinelHub's [BatchAPI](https://docs.sentinel-hub.com/api/latest/api/batch/). The data is delivered asynchronously to an S3 bucket as spacial tiles (based on the tiling scheme).\n",
    "2. There is an associated [evalscript.js](./evalscript_S2.js) & an eo-grow configuration file which can be used to specify what kind of data should be downloaded. See this link for more details : https://docs.sentinel-hub.com/api/latest/evalscript/\n",
    "![\"Batch Download\"](images/batch_download.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab70cb0-b837-4f29-b5af-1d49e930fdf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the output files of the BatchDownload Step\n",
    "- This is the first step of the pipeline but it has already been executed. We use the BatchAPI which delivers data asynchronously to S3, which can take time. \n",
    "- So, for the scope of this workshop, we've already executed and the output of this step by using the command below. The command downloads the data for Paris city in France.\n",
    "- The data includes :\n",
    "    - `batch_output` directory => Output of LoadBatch step in the \n",
    "    - `cache` => Cache generated while executing the DownloadBatch step\n",
    "    - `input-data` => The geomerty of Input AOI of Orleans region in France\n",
    "    - `training_data` => The training polygons over the Orleans region to label the training data pixels for various classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef356011-e45b-4f7c-afde-3528f9c79f1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Batch To EOPatch Step\n",
    "![\"images/batch_to_eopatch.png\"](images/batch_to_eopatch.png)\n",
    "- Here, we convert the TIFF files downloaded in the LoadBatch step to the [EO Patches](https://eo-learn.readthedocs.io/en/latest/examples/core/CoreOverview.html). \n",
    "- EO Patches are just the DataCubes of dimensions `T X H X W X C` where T = temporal dimensions, H & W are height and width of the tile, C is the bands or channels of the sensor input data. For example : we are using Sentinel-2 data (with 12 bands B1, ..., B12) for 1 year which is aggregated every two month (i.e. 6 measurements in a year). So, the in our case, the data cubes will be of dimensions `6 X H X W X 12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176706d-82f2-487b-818b-19b0a60e9581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can always run eogrow-validate to validate if our configuation file for our Pipeline is setup correctly\n",
    "!eogrow-validate config_files/batch_to_eopatch.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae767b3-ae56-4e0e-8bd3-07cc29314bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the validation ran successfully, we can run the pipeline using the eogrow command from the command line.\n",
    "!eogrow config_files/batch_to_eopatch.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7b7ce-72f3-4c2e-8853-b9687021f8ff",
   "metadata": {},
   "source": [
    "## 2.1 Let's visualize the output of one of the EOPatches that we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86441c6-409d-44d7-9f26-83cdccc1fab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, plot the distribution to have a better picture\n",
    "%matplotlib inline\n",
    "# Provide the path to the Tile that you want to visualize\n",
    "eopatch_path = f'{RUN_DIR}/eo-patch_features/raw_features/31UDQ_2_3' # 31UDQ_2_3, 31UDP_2_0\n",
    "\n",
    "# We can use the EOPatch.load function to load the data. We can always use lazy_loading when we want to.\n",
    "eopatch = EOPatch.load(eopatch_path) #, lazy_loading=True)\n",
    "\n",
    "# The following functions prints the visual bands (B2, B3, B4 <-> (blue, green, red)) for the selected EOPatch for the aggreegated six months of the year.\n",
    "# Please go ahead and try printing this for different EOPatches\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n",
    "# fig.set_size_inches((8, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow((eopatch.data['BANDS'][i, :, :, [3, 2, 1]].T / 10000) *  5.5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'Month {2*i+1} & {2*i+2} /2020')\n",
    "\n",
    "##Go back and generate this for a different tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926c2fc-e86c-4f94-86bb-b9ade3bef3da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Prepare Training\n",
    "- The goal of this step is to craft the InputFeatures which can be used for training the ML model for LC-CMS pipeline.\n",
    "- This step really depends on how the data is captured and how your ML model and workflow looks like.\n",
    "- For the purposes of this step we'll create an **new EOGrow pipeline** & and a **new EOLearn task** to load the data.\n",
    "\n",
    "We will import the custom eo-grow pipeline **PrepareTrainingDataPipeline** from the provided python file *prepare_train.py*\n",
    "![\"images/prepare_training.png\"](images/prepare_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcd075-6085-4b68-b64e-18984a84909d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the pipeline and the os module to read the file\n",
    "from prepare_train import PrepareTrainingDataPipeline\n",
    "\n",
    "# First, we need to read the config file for the pipeline\n",
    "config_path = os.path.join('config_files', 'prepare_train.json')\n",
    "\n",
    "# Then we can build an eo-grow Pipeline object for this pipeline as follows :\n",
    "pre_train_pipeline = PrepareTrainingDataPipeline.from_path(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0e5f1-d290-45c2-938d-54c614d169cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, we can run this pipeline using the run() method on the pipeline.\n",
    "pre_train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0839e-eef7-4fe2-b645-d7d8ba9ace8e",
   "metadata": {},
   "source": [
    " **The output of this step is two ND-arrays** :\n",
    " - `training_features.npy` => Stores the Crafted input features\n",
    " - `training_labels.npy` => Stores the labels correspoinding to all the training features\n",
    " \n",
    "Lets calculate some statistics about out consolidated training data before we train our ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ef5c4-dae4-4a88-a1ae-197613f90b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training features & labels from the training_data directory\n",
    "training_data = np.load(f'{RUN_DIR}/training_data/training_features.npy')\n",
    "training_labels = np.load(f'{RUN_DIR}/training_data/training_labels.npy')\n",
    "print(training_data.shape, training_labels.shape)\n",
    "# Calculate the distribution of labels\n",
    "label_distribution = np.unique(training_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998e4fa-fcb5-439c-a5b3-72fdbccda635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, plot the distribution to have a better picture\n",
    "%matplotlib inline\n",
    "target_names = [LABELS_ORIG[l] for l in label_distribution[0]]\n",
    "ax = sns.barplot(y=label_distribution[1], x=target_names, orient='v')\n",
    "ax.set(title='Pixels distribution of available LC Classes', ylabel='Pixels', xlabel='LC classes')\n",
    "ax.bar_label(ax.containers[0], labels=label_distribution[1])\n",
    "# Hide the right and top spines\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.figure.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1477938-2f8c-4753-8b8b-705c9d98b863",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badbf4c-00ae-452a-a5b2-92caacbce0be",
   "metadata": {},
   "source": [
    "**It time to train our LC Classificaiton model.**\n",
    "- We use a LightGBM Random Forest Classification model.\n",
    "- For this workshop, we will use the ***ClassificationTrainingPipeline*** which already comes with the eo-grow package : *eogrow.pipelines.training*\n",
    "- If needed, we can modify the model parameters like num_classes, weight by modifying the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832a31e-24e0-43c2-a49c-7ab6cfd9b4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training pipeline step\n",
    "!eogrow config_files/training.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fab535-454f-4b3a-8c1b-28a931c29881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can now load our saved model and generate some stats\n",
    "_, val_data, _, val_labels = train_test_split(training_data, training_labels, random_state=99, train_size=0.8, stratify=training_labels)\n",
    "print(val_data.shape, val_labels.shape)\n",
    "\n",
    "# Load the LGBMClassifier and the LabelEncoder which are saved as an output of training step\n",
    "with open(f'{RUN_DIR}/model/lc-cms-LE.pkl', 'rb') as f:\n",
    "    label_encoder: LabelEncoder = joblib.load(f)\n",
    "with open(f'{RUN_DIR}/model/lc-cms_model.pkl', 'rb') as f:\n",
    "    model: LGBMClassifier = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a73755-4cc2-4beb-809f-e8bd96317df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the classification report on the Validation dataset\n",
    "val_preds = model.predict(val_data)\n",
    "print(classification_report(val_labels, label_encoder.inverse_transform(val_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1cdc1-cc48-4eef-9eaa-c0d1293cb48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can also have a look at the Feature Importance matrix to see which features player more significant role while predicting\n",
    "%matplotlib inline\n",
    "y_labels = [f'T{i}' for i in range(1, (len(model.feature_importances_) // len(INPUT_BANDS)) + 1)]\n",
    "ax = sns.heatmap(model.feature_importances_.reshape(len(y_labels), len(INPUT_BANDS)), annot=False, cmap=plt.cm.Greens,\n",
    "                     xticklabels=INPUT_BANDS,\n",
    "                     yticklabels=y_labels)\n",
    "ax.set(title='Feature Importance Matrix')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1df18-f611-44e3-9c94-35e36afb4cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lastly, we can also have a look at ROC curve to see the bevavious of predictions for different thresholds\n",
    "%matplotlib inline\n",
    "val_proba = model.predict_proba(val_data)\n",
    "\n",
    "display_labels = [LABELS_ORIG[l] for l in label_distribution[0]]\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Base')\n",
    "for i in range(4):\n",
    "    fpr, tpr, thresholds = roc_curve(label_encoder.transform(val_labels), val_proba[:, i], pos_label=i)\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    # print(fpr)\n",
    "    print(f'Best Threshold for {display_labels[i]} {thresholds[ix]:0.3f}, G-Mean={gmeans[ix]:0.2f}')\n",
    "    plt.scatter(fpr[ix], tpr[ix], marker='X', color='black', zorder=10)    \n",
    "    plt.plot(fpr, tpr, marker='.', label=display_labels[i])\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26e0a1-5a10-4318-868a-78b6d3a0a97b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Prediction\n",
    "Now, that everythink looks good and if we are confident that model is behaving as expected, we can start performing predictions for all the EOPatches using the trained model.\n",
    "- For this purpose, will use the **ClassificationPredictionPipeline** from the *eogrow.pipelines.prediction* module of *eo-grow*\n",
    "- It performs the prediction & calculates class probabilities on every EOPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e01d5-e9da-43fa-8e4c-0fc81f18eb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the prediction step\n",
    "!eogrow config_files/prediction.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2691cfb-7bf3-4545-95a9-e924c5284caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Exporting & Serving\n",
    "* This step is used to change the output from the EOPatch format back to the tiff or the desired output format. It can be thought of inverse of the first BatchToEOPatch Step.\n",
    "* In addition to exporting, it also concatenates the output per UTM zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f165f4c-c7ba-4174-b017-20e055f6e321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's go ahead and run this step\n",
    "!eogrow config_files/export_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8d349-afb9-4781-b692-e190f225eb06",
   "metadata": {},
   "source": [
    "# 7. Summary\n",
    "* We executed all the steps in Green color\n",
    "* All the steps in grey boxes were provided already\n",
    "* We can now use the output of this pipeline for downstream processes for example Change Detection\n",
    "![\"images/conclusion.png\"](images/conclusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b4985-5380-48c1-b79e-ad25d37fa2ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. Change Detection [Optional]\n",
    "* There are various approaches to detecting change and in this artilce we are going to look at one of them which is monitoring changes using the predicted output.\n",
    "* This can be simply done by taking a diff of predictions between two time periods of predictions.\n",
    "![\"images/change_detection.png\"](images/change_detection.png)\n",
    "\n",
    "### The **results of LC-CMS & Change Detection** can be viewed at the following link : https://www.globalearthmonitor.eu/sites/default/files/LC_CMS/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adab48-c921-4418-800b-1bd6304ff9e4",
   "metadata": {},
   "source": [
    "**We can now run the LC-CMS pipeline for different year (year 2021). The training data we'll assume remains the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f1651-dfd0-46ce-bfa6-ab836cb747b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, change the project_dir in the global configuration to the new 2021 project_dir that we want to perform the run for.\n",
    "# Also, change the RUN_DIR so that everything is referenced properly\n",
    "RUN_DIR = '/gem-workshop/lccms_rundir/paris_run/2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3c32c-0409-4bf3-963d-4827e7f5fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the steps one by one\n",
    "# We first run the Batch To EOPatch pipeline step\n",
    "!eogrow config_files/batch_to_eopatch.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7b82a-00e1-4b87-8306-edb09d81f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PrepareTraining step \n",
    "config_path = os.path.join('config_files', 'prepare_train.json')\n",
    "# Then we can build an eo-grow Pipeline object for this pipeline as follows :\n",
    "prepare_train_pipeline = PrepareTrainingDataPipeline.from_path(config_path)\n",
    "prepare_train_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb936de-f7e5-461a-9c62-e33b27c29df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training step\n",
    "!eogrow config_files/training.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7968c6-21b4-4c90-aa59-1471f5794053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction step\n",
    "!eogrow config_files/prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc677-deab-4245-8ba8-38f489bb15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Export Predictions step\n",
    "!eogrow config_files/export_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064b36d-25bc-4efa-8110-5882eac19108",
   "metadata": {},
   "source": [
    "**Now that we have the prediction for both 2020 & 2021, we can take the diff between the predictios to detect the change**\n",
    "\n",
    "*The `generate_change_detection_using_diff` function can generate the Diff of the predictions between 2020 & 2021 for us.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18185b0a-09c4-45e3-8b8c-73c73800044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_change_detection_using_diff(from_preds_path, to_preds_path, diff_path):\n",
    "    \"\"\"\n",
    "    This function generates the diff between the the LC-CMS predictions taken over two different time periods.\n",
    "    For example, if we can generate change b/w  predictions of year 2020 and 2021.\n",
    "    Both input and output should be of same size.\n",
    "    Parameters\n",
    "    ----------\n",
    "    from_preds_path : Predictions from which diff should be taken\n",
    "    to_preds_path : Predictions to which diff should be taken\n",
    "    diff_path : Path where diff should be stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    with rasterio.open(from_preds_path) as f:\n",
    "        preds_from = f.read()\n",
    "        input_profile = f.profile\n",
    "        with rasterio.open(to_preds_path) as f:\n",
    "            preds_to = f.read()\n",
    "            pred_diff = np.where(preds_from != preds_to, preds_to, 255.)\n",
    "            with rasterio.open(diff_path, 'w', **input_profile) as f:\n",
    "                f.write(pred_diff)\n",
    "                print(f'Successfully generated prediction at location => {diff_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebd26c-f391-40fc-86ac-919afba055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can call this function with from year, to year and the diff_path where we need to store the output :\n",
    "generate_change_detection_using_diff(from_preds_path='/gem-workshop/lccms_rundir/paris_run/2020/serve/UTM_32631/PRED.tiff', \n",
    "                                     to_preds_path='/gem-workshop/lccms_rundir/paris_run/2021/serve/UTM_32631/PRED.tiff', \n",
    "                                     diff_path='/gem-workshop/lccms_rundir/paris_run/change_2020_2021.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186287f-5eea-4f23-8d0b-1a5a4db02a82",
   "metadata": {},
   "source": [
    "**This output can be imported in the QGIS now.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
